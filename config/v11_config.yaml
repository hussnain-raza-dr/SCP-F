# Improved cGAN Configuration (v11)
# Finding the balance between D dominance (v9) and G dominance (v10).
#
# v9: D crushed G (D=64 base, TTUR 4x, n_critic=2) → Tanh saturation
# v10: G crushed D (D=32 base, equal LR, n_critic=1) → D can't learn
#      D_loss stuck at ~2.0 (hinge max), Real→Real acc < 50%
#
# v11 splits the difference:
#   - D base_channels=48 (midpoint: ~5.5M D params vs 2.5M G, ratio ~0.45)
#   - Mild TTUR: lr_d = 2×lr_g (not 4x like v9, not 1x like v10)
#   - n_critic=1 (keep from v10 — D gets mild LR advantage instead)
#   - Tanh saturation fix retained (no SN on G final conv, 0.1x init)
#   - Instance noise retained from v10

model:
  architecture: improved
  latent_dim: 128
  embed_dim: 64
  generator_base_channels: 256
  discriminator_base_channels: 48    # Midpoint: 32 was too weak, 64 too strong
  use_self_attention: true
  use_residual: true

training:
  batch_size: 128
  num_epochs: 200
  lr_g: 0.0001                       # Mild TTUR: D learns 2x faster than G
  lr_d: 0.0002                       # (v9 was 4x, v10 was 1x)
  beta1: 0.0
  beta2: 0.999
  n_critic: 1                        # Keep at 1 — TTUR provides D advantage
  loss_type: "hinge"
  grad_clip: 0.0

  # EMA
  ema_decay: 0.999

  # No LR schedule initially
  lr_schedule: "none"

  # Instance noise — extended to full training duration
  d_noise_std: 0.1                   # Reduced from 0.15 (D needs to see cleaner images)
  d_noise_decay_epochs: 150          # Compromise: not 100 (v9) or 200 (v10)

  # R1 gradient penalty: still disabled
  r1_gamma: 0.0
  r1_every: 16

data:
  num_classes: 10
  image_size: 32
  seed: 42
  num_workers: 2
