# Improved cGAN Configuration (v10)
# Fixes for Tanh saturation and D dominance observed in v9.
#
# v9 issues:
#   1. Complete Tanh saturation from epoch 1 — all pixels pushed to ±1
#      producing only 8 pure colors (R,G,B,C,M,Y,W,K)
#   2. D dominance: 9.8M params vs G's 2.5M (ratio 0.25), compounded by
#      D-favored TTUR (lr_d = 4×lr_g) and n_critic=2
#   3. Cosine LR decay killed G learning rate before quality improved
#
# v10 fixes:
#   - D base_channels 64→32 (reduces D from 9.8M to ~2.5M, G/D ratio ~1.0)
#   - Equal LR (2e-4 for both G and D) — no TTUR needed when G/D balanced
#   - n_critic=1 — D already powerful enough with SN + residual blocks
#   - No LR schedule — let G learn throughout training
#   - Instance noise extended to full 200 epochs
#   - Generator final conv init scaled by 0.1x to prevent Tanh saturation
#     (requires corresponding code change in generator.py _init_weights)

model:
  architecture: improved
  latent_dim: 128
  embed_dim: 64
  generator_base_channels: 256
  discriminator_base_channels: 32    # Halved from 64 to balance G/D capacity
  use_self_attention: true
  use_residual: true

training:
  batch_size: 128
  num_epochs: 100
  lr_g: 0.0002                       # Equal to lr_d — balanced training
  lr_d: 0.0002                       # Reduced from 0.0004
  beta1: 0.0
  beta2: 0.999
  n_critic: 1                        # Reduced from 2 — D doesn't need extra steps
  loss_type: "hinge"
  grad_clip: 0.0

  # EMA
  ema_decay: 0.999

  # No LR schedule — constant LR lets G learn throughout training.
  # Reintroduce cosine decay only after confirming generation works.
  lr_schedule: "none"

  # Instance noise — extended to full training duration
  d_noise_std: 0.15                  # Increased from 0.1
  d_noise_decay_epochs: 200          # Extended from 100

  # R1 gradient penalty: still disabled
  r1_gamma: 0.0
  r1_every: 16

data:
  num_classes: 10
  image_size: 32
  seed: 42
  num_workers: 2
