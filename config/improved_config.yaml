# Improved cGAN Configuration (v6 — calibrated R1)
# Architecture: SN-GAN style with hinge loss.
#
# v5 results: epoch 20 images were the BEST yet (real shapes & textures!),
# proving the architecture + reduced D capacity work. But R1 penalty with
# lazy scaling (gamma/2 * r1_every * ||grad||²) created 16x gradient spikes
# that killed D around epoch 25 → G loss diverged to -3e8.
#
# v6 fix: Remove lazy scaling multiplier. R1 is now applied every 16 steps
# with constant weight (gamma/2), avoiding destructive spikes. Gamma
# increased to 2.0 to compensate for the less frequent application.

model:
  architecture: improved
  latent_dim: 128
  embed_dim: 64
  generator_base_channels: 256
  discriminator_base_channels: 48    # Reduced from 64 → ~5.5M params (was ~9.8M)
  use_self_attention: true
  use_residual: true

training:
  batch_size: 128
  num_epochs: 200
  # G-favored TTUR
  lr_g: 0.0002
  lr_d: 0.0001
  beta1: 0.0
  beta2: 0.9
  n_critic: 1
  loss_type: "hinge"
  grad_clip: 0.0

  # EMA
  ema_decay: 0.999

  # Cosine LR scheduling
  lr_schedule: "cosine"

  # Instance noise
  d_noise_std: 0.1
  d_noise_decay_epochs: 100

  # R1 gradient penalty: applied every r1_every steps WITHOUT lazy scaling.
  # Effective penalty per application = gamma/2 * ||∇D(real)||² (no multiplier).
  r1_gamma: 2.0          # Higher gamma to compensate for no lazy scaling
  r1_every: 16            # Compute every 16 steps (for efficiency, not scaling)

data:
  num_classes: 10
  image_size: 32
  seed: 42
  num_workers: 2
