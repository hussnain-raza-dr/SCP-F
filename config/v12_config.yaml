# Improved cGAN Configuration (v12)
# Fix Tanh saturation from v11 while keeping its balanced training dynamics.
#
# v11 recap: Balanced D/G dynamics (losses ~1.0, Real→Real ~85-90%) but
#   generator outputs fully saturate Tanh — pure cyan/white/black/yellow
#   pixels. The 0.1 weight init on the final conv was only an initial
#   constraint; the optimizer grew those weights during training, pushing
#   pre-Tanh activations to ±10+ where Tanh gradients vanish.
#
# v12 changes:
#   - Spectral norm on G's final conv (persistent ||W||_op=1 constraint).
#     With BN-normalized inputs, this bounds pre-Tanh std to ≤1, keeping
#     activations in Tanh's responsive range throughout training.
#   - Gradient clipping (max_norm=1.0) as additional safety net.
#   - All other hyperparameters unchanged from v11.

model:
  architecture: improved
  latent_dim: 128
  embed_dim: 64
  generator_base_channels: 256
  discriminator_base_channels: 48    # Midpoint: 32 was too weak, 64 too strong
  use_self_attention: true
  use_residual: true

training:
  batch_size: 128
  num_epochs: 200
  lr_g: 0.0001                       # Mild TTUR: D learns 2x faster than G
  lr_d: 0.0002                       # (v9 was 4x, v10 was 1x)
  beta1: 0.0
  beta2: 0.999
  n_critic: 1                        # Keep at 1 — TTUR provides D advantage
  loss_type: "hinge"
  grad_clip: 1.0                     # NEW: prevent unbounded weight growth

  # EMA
  ema_decay: 0.999

  # No LR schedule initially
  lr_schedule: "none"

  # Instance noise — extended to full training duration
  d_noise_std: 0.1                   # Reduced from 0.15 (D needs to see cleaner images)
  d_noise_decay_epochs: 150          # Compromise: not 100 (v9) or 200 (v10)

  # R1 gradient penalty: still disabled
  r1_gamma: 0.0
  r1_every: 16

data:
  num_classes: 10
  image_size: 32
  seed: 42
  num_workers: 2
